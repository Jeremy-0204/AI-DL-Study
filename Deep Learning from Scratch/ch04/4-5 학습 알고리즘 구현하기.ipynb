{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 학습 알고리즘 구현하기\n",
    "\n",
    "신경망 학습의 절차를 복습해보자:  \n",
    "전체) 신경망에는 적용가능한 가중치와 편향이 있고, 이들을 훈련 데이터에 적응하도록 조정하는 과정을 학습이라 한다.  \n",
    "\n",
    "1) 미니배치  \n",
    "훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하여 그 손실 함수 값을 줄이는 것이 목표이다.  \n",
    "\n",
    "2) 기울기 산출  \n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.  \n",
    "\n",
    "3) 매개변수 갱신  \n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.  \n",
    "\n",
    "4) 반복\n",
    "1 ~ 3단계를 반복한다.  \n",
    "\n",
    "이때 무작위로 미니배치를 선정하기에 확률적 경사 하강법 (sochastic gradient descent)라고 부른다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    # 초기화를 수행한다.\n",
    "    # 인수는 입력층, 은닉층, 출력층의 뉴런 수\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}    # 매개변수를 저장할 딕셔너리 변수 (인스턴스 변수)\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)     # 첫번째 층의 가중치\n",
    "        self.params['b1'] = np.zeros(hidden_size)   # 첫번째 층의 편향\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)    # 두번째 층의 가중치\n",
    "        self.params['b2'] = np.zeros(output_size)   # 두번째 층의 편향\n",
    "        \n",
    "    # 실제 추론을 하는 곳\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}    # 기울기를 보관하는 딕셔너리 변수 (numerical_gradient() 메서드의 반환 값)\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1']) # 각 층의 가중치와 편향의 기울기를 계산해서 저장한다.\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# 만든 클래스는 prams, grads라는 인스턴스 변수로 갖는다.\n",
    "\n",
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784) # 더미 데이터 (100장 분량)\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784) # 더미 데이터 (100장 분량)\n",
    "t = np.random.rand(100, 10) # 더미 데이터 (100장 분량)\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # 기울기 계산\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
